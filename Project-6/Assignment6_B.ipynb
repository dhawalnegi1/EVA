{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment6-B.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhawalnegi1/EVA/blob/master/Project-6/Assignment6_B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy5UW6qKmEfj",
        "colab_type": "text"
      },
      "source": [
        "#The Model using function api having normal, spatial sperable, depthwise seperable and grouped convolution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps4HLdNgmR0G",
        "colab_type": "text"
      },
      "source": [
        "Import all requirements(Input, merge, SeperableConv2D, Concatenate are new inclusions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkwXnw9OfHZl",
        "colab_type": "code",
        "outputId": "08d5a22d-b311-4d14-818c-63fb4cd00458",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras import backend as K\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "% matplotlib inline\n",
        "np.random.seed(2017)\n",
        "\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation,SeparableConv2D\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Reshape,Input,Lambda\n",
        "from keras.layers.merge import concatenate"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-mZkGZPmnc4",
        "colab_type": "text"
      },
      "source": [
        "Import the cifar dataset and store in the variables for training and test purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHpnoCHZfO8g",
        "colab_type": "code",
        "outputId": "5298c324-b2ac-4f57-be07-4b50eb2df2ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load CIFAR10 Data\n",
        "from keras.datasets import cifar10\n",
        "(xtrain, ytrain), (xtest, ytest) = cifar10.load_data()\n",
        "img_height, img_width, channel = xtrain.shape[1],xtrain.shape[2],xtrain.shape[3]\n",
        "num_classes = len(np.unique(ytest))\n",
        "print(num_classes)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK9qBBDDmyTn",
        "colab_type": "text"
      },
      "source": [
        "Function accuracy is defined to calculate the validation accuracy by finding total no of images classfied correctly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Adhd5AMrD3RP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(xtest, ytest, model):\n",
        "    result = model.predict(xtest)\n",
        "    predicted_class = np.argmax(result, axis=1)\n",
        "    true_class = np.argmax(ytest, axis=1)\n",
        "    num_correct = np.sum(predicted_class == true_class) \n",
        "    accuracy = float(num_correct)/result.shape[0]\n",
        "    return (accuracy * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxjI7Pl8m0a6",
        "colab_type": "text"
      },
      "source": [
        "Pixel Normalisation is done for training and testing data. And the class labels for training and testing data is converted into one-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5c5nDvxm6zR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xtrain = xtrain.astype('float32')/255\n",
        "xtest = xtest.astype('float32')/255\n",
        "# convert class labels to binary class labels\n",
        "ytrain = np_utils.to_categorical(ytrain, num_classes)\n",
        "ytest = np_utils.to_categorical(ytest, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l2Ey2mAm3DS",
        "colab_type": "text"
      },
      "source": [
        "##Block1 - Normal Convolution\n",
        "\n",
        "2 layers of normal 3x3 convolution is add and after that bottleneck layer of 1x1 and maxpooling is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q5Rxgakv4SG",
        "colab_type": "code",
        "outputId": "6c14affd-84c1-4712-f69a-0a3b7ae8bd59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "input = Input(shape=(img_height, img_width, channel,))\n",
        "\n",
        "# BLOCK 1\n",
        "L11 = Conv2D(32, (3,3), strides=(1,1), name='normalconv1', use_bias=False,padding='same')(input)\n",
        "L11 = BatchNormalization()(L11)\n",
        "L11 = Activation('relu')(L11)\n",
        "\n",
        "L12 = Conv2D(64, (3,3), strides=(1,1), name='normalconv2', use_bias=False)(L11)\n",
        "L12 = BatchNormalization()(L12)\n",
        "L12 = Activation('relu')(L12)\n",
        "\n",
        "L13 = Conv2D(16,1,use_bias=False)(L12)\n",
        "L13 = MaxPooling2D(pool_size=(2, 2))(L13)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWQAaYgDnXY0",
        "colab_type": "text"
      },
      "source": [
        "##Block2 - Spatoally Seperable Convolution\n",
        "2 layers of Spatially Separable Convolution(3x1 and 1x3) is add and after that bottleneck layer of 1x1 and maxpooling is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt_LFnoZwS-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#BLOCK 2\n",
        "L21 = Conv2D(16, (3,1), strides=(1,1), name='spatialseperable1-3-1', use_bias=False)(L13)\n",
        "L21 = BatchNormalization()(L21)\n",
        "L21 = Activation('relu')(L21)\n",
        "L21 = Conv2D(32, (1,3), strides=(1,1), name='spatialseperable1-1-3', use_bias=False)(L21)\n",
        "L21 = BatchNormalization()(L21)\n",
        "L21 = Activation('relu')(L21)\n",
        "\n",
        "L22 = Conv2D(32, (3,1), strides=(1,1), name='spatialseperable2-3-1', use_bias=False)(L21)\n",
        "L22 = BatchNormalization()(L22)\n",
        "L22 = Activation('relu')(L22)\n",
        "L22 = Conv2D(64, (1,3), strides=(1,1), name='spatialseperable2-1-3', use_bias=False)(L22)\n",
        "L22 = BatchNormalization()(L22)\n",
        "L22 = Activation('relu')(L22)\n",
        "\n",
        "L23 = Conv2D(32,1,use_bias=False)(L22)\n",
        "L23 = MaxPooling2D(pool_size=(2, 2))(L23)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uck5M9GZoMBu",
        "colab_type": "text"
      },
      "source": [
        "#Block3 - Depthwise Separable Convolution\n",
        "1 Depthwise Separable Convolution layer is apllied which first seperate all the input channel and then apply 3x3 on each individually and then 1x1 is applied to combine them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bamEVCBHxLMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "L3 = SeparableConv2D(64, (3,3), strides=(1, 1), depth_multiplier=1, use_bias=False, name='depthwise-seperable')(L23)\n",
        "L3 = BatchNormalization()(L3)\n",
        "L3 = Activation('relu')(L3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4KITiamoxMw",
        "colab_type": "text"
      },
      "source": [
        "#Block4 - Grouped Convolution (use 3x3, 5x5 only)\n",
        "The group convolution means 2 diffent set of convolution is applied on input layer in parallel.\n",
        "3x3 is applied on L3. and parallely 2 3x3 are applied , and then output of both convolution are concatneted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5OFdGFR7GIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "L41 = Conv2D(128, (3,3), strides=(1,1), name='3x3', use_bias=False)(L3)\n",
        "L41 = BatchNormalization()(L41)\n",
        "L41 = Activation('relu')(L41)\n",
        "\n",
        "L42 = Conv2D(64, (3,3), strides=(1,1), name='5x5-1',padding='same', use_bias=False)(L3)\n",
        "L42 = BatchNormalization()(L42)\n",
        "L42 = Activation('relu')(L42)\n",
        "L42 = Conv2D(128, (3,3), strides=(1,1), name='5x5-2', use_bias=False)(L42)\n",
        "L42 = BatchNormalization()(L42)\n",
        "L42 = Activation('relu')(L42)\n",
        "\n",
        "L4 = concatenate([L41,L42])\n",
        "\n",
        "L4 = Conv2D(32,1)(L4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U9R5Fx9patw",
        "colab_type": "text"
      },
      "source": [
        "#Block5 - Grouped Convolution (3x3 with dilation of 1 and 2)\n",
        "The group convolution means 2 diffent set of convolution is applied on input layer in parallel.\n",
        "3x3 with dilation 1 is applied on L4. and parallely 3x3 with dilation of 2 is applied , and then output of both convolution are concatneted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hnHRpjk8E7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "L51 = Conv2D(64, (3,3), strides=(1,1), name='dilation-1',padding='same', use_bias=False,dilation_rate=(1, 1))(L4)\n",
        "L51 = BatchNormalization()(L51)\n",
        "L51 = Activation('relu')(L51)\n",
        "\n",
        "L52 = Conv2D(64, (3,3), strides=(1,1), name='dilation-2',padding='same', use_bias=False,dilation_rate=(2, 2))(L4)\n",
        "L52 = BatchNormalization()(L52)\n",
        "L52 = Activation('relu')(L52)\n",
        "\n",
        "L5 = concatenate([L51,L52])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syzaQVIfpxZy",
        "colab_type": "text"
      },
      "source": [
        "Finally the no of channels are reduced to 10 and flatten and softmax is applied."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpp2vTc8EN_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "OUTPUT = Conv2D(10,1)(L5)\n",
        "OUTPUT = Flatten()(OUTPUT)\n",
        "Output = Activation('softmax')(OUTPUT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSOb2lkJfhVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile the model\n",
        "model= Model(inputs=[input],outputs=[Output])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fh77A9fqBLC",
        "colab_type": "text"
      },
      "source": [
        "Model is compiled and summery is printed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvVs5tC8mR6Q",
        "colab_type": "code",
        "outputId": "a11c1c67-e8c1-46ce-85d2-04eb83fff857",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1768
        }
      },
      "source": [
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "normalconv1 (Conv2D)            (None, 32, 32, 32)   864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 32)   128         normalconv1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 32)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "normalconv2 (Conv2D)            (None, 30, 30, 64)   18432       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 30, 30, 64)   256         normalconv2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 30, 30, 64)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 30, 30, 16)   1024        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 15, 15, 16)   0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "spatialseperable1-3-1 (Conv2D)  (None, 13, 15, 16)   768         max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 13, 15, 16)   64          spatialseperable1-3-1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 13, 15, 16)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "spatialseperable1-1-3 (Conv2D)  (None, 13, 13, 32)   1536        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 13, 13, 32)   128         spatialseperable1-1-3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 13, 13, 32)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "spatialseperable2-3-1 (Conv2D)  (None, 11, 13, 32)   3072        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 11, 13, 32)   128         spatialseperable2-3-1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 11, 13, 32)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "spatialseperable2-1-3 (Conv2D)  (None, 11, 11, 64)   6144        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 11, 11, 64)   256         spatialseperable2-1-3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 11, 11, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 11, 11, 32)   2048        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 5, 5, 32)     0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "depthwise-seperable (SeparableC (None, 3, 3, 64)     2336        max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 3, 3, 64)     256         depthwise-seperable[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 3, 3, 64)     0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "5x5-1 (Conv2D)                  (None, 3, 3, 64)     36864       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 3, 3, 64)     256         5x5-1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 3, 3, 64)     0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "3x3 (Conv2D)                    (None, 1, 1, 128)    73728       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "5x5-2 (Conv2D)                  (None, 1, 1, 128)    73728       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 1, 1, 128)    512         3x3[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 1, 1, 128)    512         5x5-2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 1, 1, 128)    0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 1, 1, 128)    0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 1, 1, 256)    0           activation_8[0][0]               \n",
            "                                                                 activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 1, 1, 32)     8224        concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dilation-1 (Conv2D)             (None, 1, 1, 64)     18432       conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dilation-2 (Conv2D)             (None, 1, 1, 64)     18432       conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 1, 1, 64)     256         dilation-1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 1, 1, 64)     256         dilation-2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 1, 1, 64)     0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 1, 1, 64)     0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 1, 1, 128)    0           activation_11[0][0]              \n",
            "                                                                 activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 1, 1, 10)     1290        concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 10)           0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 10)           0           flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 269,930\n",
            "Trainable params: 268,426\n",
            "Non-trainable params: 1,504\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3gHMtq1p_5J",
        "colab_type": "text"
      },
      "source": [
        "Model is Trained for 50 epocs and checkpoint is used to saved the model with best validation accuracy and time taken to run is calculated. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oK_V7WyfsXX",
        "colab_type": "code",
        "outputId": "c82b5a27-de47-4870-81af-3dd7121f4eb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3539
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(zoom_range=0.0, \n",
        "                             horizontal_flip=False)\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
        "# train the model\n",
        "start = time.time()\n",
        "# Train the model\n",
        "filepath=\"saved.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "model_info = model.fit_generator(datagen.flow(xtrain, ytrain, batch_size = 128),\n",
        "                                 samples_per_epoch = xtrain.shape[0], nb_epoch = 50, \n",
        "                                 validation_data = (xtest,ytest),callbacks= callbacks_list, verbose=1)\n",
        "end = time.time()\n",
        "print (\"Model took %0.2f seconds to train\"%(end - start))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=(array([[[..., callbacks=[<keras.ca..., verbose=1, steps_per_epoch=390, epochs=50)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "390/390 [==============================] - 13s 33ms/step - loss: 1.5521 - acc: 0.4310 - val_loss: 1.7086 - val_acc: 0.4227\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.42270, saving model to saved.hdf5\n",
            "Epoch 2/50\n",
            "390/390 [==============================] - 10s 24ms/step - loss: 1.2110 - acc: 0.5651 - val_loss: 1.6198 - val_acc: 0.4390\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.42270 to 0.43900, saving model to saved.hdf5\n",
            "Epoch 3/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 1.0424 - acc: 0.6274 - val_loss: 1.1313 - val_acc: 0.5974\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.43900 to 0.59740, saving model to saved.hdf5\n",
            "Epoch 4/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.9287 - acc: 0.6672 - val_loss: 1.1201 - val_acc: 0.6072\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.59740 to 0.60720, saving model to saved.hdf5\n",
            "Epoch 5/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.8439 - acc: 0.6980 - val_loss: 0.9871 - val_acc: 0.6469\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.60720 to 0.64690, saving model to saved.hdf5\n",
            "Epoch 6/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.7743 - acc: 0.7253 - val_loss: 0.9791 - val_acc: 0.6545\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.64690 to 0.65450, saving model to saved.hdf5\n",
            "Epoch 7/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.7137 - acc: 0.7460 - val_loss: 1.1069 - val_acc: 0.6420\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.65450\n",
            "Epoch 8/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.6610 - acc: 0.7633 - val_loss: 0.9870 - val_acc: 0.6727\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.65450 to 0.67270, saving model to saved.hdf5\n",
            "Epoch 9/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.6048 - acc: 0.7848 - val_loss: 1.0849 - val_acc: 0.6510\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.67270\n",
            "Epoch 10/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.5602 - acc: 0.7997 - val_loss: 1.0351 - val_acc: 0.6660\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.67270\n",
            "Epoch 11/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.5105 - acc: 0.8156 - val_loss: 1.0337 - val_acc: 0.6755\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.67270 to 0.67550, saving model to saved.hdf5\n",
            "Epoch 12/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.4716 - acc: 0.8303 - val_loss: 1.0369 - val_acc: 0.6822\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.67550 to 0.68220, saving model to saved.hdf5\n",
            "Epoch 13/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.4207 - acc: 0.8505 - val_loss: 1.0790 - val_acc: 0.6655\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.68220\n",
            "Epoch 14/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.3895 - acc: 0.8599 - val_loss: 1.0002 - val_acc: 0.6989\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.68220 to 0.69890, saving model to saved.hdf5\n",
            "Epoch 15/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.3451 - acc: 0.8756 - val_loss: 1.1647 - val_acc: 0.6772\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.69890\n",
            "Epoch 16/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.3238 - acc: 0.8848 - val_loss: 1.3900 - val_acc: 0.6525\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.69890\n",
            "Epoch 17/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.2929 - acc: 0.8943 - val_loss: 1.1256 - val_acc: 0.6968\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.69890\n",
            "Epoch 18/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.2603 - acc: 0.9067 - val_loss: 1.2070 - val_acc: 0.6877\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.69890\n",
            "Epoch 19/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.2469 - acc: 0.9095 - val_loss: 1.2205 - val_acc: 0.6873\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.69890\n",
            "Epoch 20/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.2147 - acc: 0.9226 - val_loss: 1.4674 - val_acc: 0.6709\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.69890\n",
            "Epoch 21/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.2079 - acc: 0.9240 - val_loss: 1.2982 - val_acc: 0.6805\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.69890\n",
            "Epoch 22/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.1883 - acc: 0.9330 - val_loss: 1.3361 - val_acc: 0.6874\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.69890\n",
            "Epoch 23/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.1744 - acc: 0.9369 - val_loss: 1.3926 - val_acc: 0.6912\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.69890\n",
            "Epoch 24/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.1621 - acc: 0.9412 - val_loss: 1.4685 - val_acc: 0.6809\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.69890\n",
            "Epoch 25/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.1585 - acc: 0.9433 - val_loss: 1.4048 - val_acc: 0.6891\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.69890\n",
            "Epoch 26/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.1481 - acc: 0.9463 - val_loss: 1.5285 - val_acc: 0.6737\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.69890\n",
            "Epoch 27/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.1385 - acc: 0.9500 - val_loss: 1.4847 - val_acc: 0.6928\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.69890\n",
            "Epoch 28/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.1292 - acc: 0.9535 - val_loss: 1.4641 - val_acc: 0.6952\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.69890\n",
            "Epoch 29/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.1363 - acc: 0.9517 - val_loss: 1.5783 - val_acc: 0.6839\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.69890\n",
            "Epoch 30/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.1289 - acc: 0.9532 - val_loss: 1.5032 - val_acc: 0.7005\n",
            "\n",
            "Epoch 00030: val_acc improved from 0.69890 to 0.70050, saving model to saved.hdf5\n",
            "Epoch 31/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.1175 - acc: 0.9578 - val_loss: 1.6144 - val_acc: 0.6831\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.70050\n",
            "Epoch 32/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.1092 - acc: 0.9607 - val_loss: 1.5950 - val_acc: 0.6878\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.70050\n",
            "Epoch 33/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.1072 - acc: 0.9621 - val_loss: 1.5993 - val_acc: 0.6929\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.70050\n",
            "Epoch 34/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.1152 - acc: 0.9592 - val_loss: 1.6856 - val_acc: 0.6899\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.70050\n",
            "Epoch 35/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.1034 - acc: 0.9630 - val_loss: 1.6083 - val_acc: 0.6890\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.70050\n",
            "Epoch 36/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.1019 - acc: 0.9634 - val_loss: 1.7261 - val_acc: 0.6876\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.70050\n",
            "Epoch 37/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.1001 - acc: 0.9640 - val_loss: 1.6579 - val_acc: 0.6865\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.70050\n",
            "Epoch 38/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.0972 - acc: 0.9657 - val_loss: 1.6132 - val_acc: 0.6984\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.70050\n",
            "Epoch 39/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.0915 - acc: 0.9676 - val_loss: 1.7085 - val_acc: 0.6871\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.70050\n",
            "Epoch 40/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.0849 - acc: 0.9702 - val_loss: 1.7238 - val_acc: 0.6938\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.70050\n",
            "Epoch 41/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.0895 - acc: 0.9695 - val_loss: 1.6454 - val_acc: 0.6970\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.70050\n",
            "Epoch 42/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.0938 - acc: 0.9659 - val_loss: 1.6578 - val_acc: 0.6956\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.70050\n",
            "Epoch 43/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.0882 - acc: 0.9676 - val_loss: 1.7092 - val_acc: 0.6915\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.70050\n",
            "Epoch 44/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.0749 - acc: 0.9734 - val_loss: 1.7812 - val_acc: 0.6870\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.70050\n",
            "Epoch 45/50\n",
            "390/390 [==============================] - 10s 26ms/step - loss: 0.0841 - acc: 0.9699 - val_loss: 1.6711 - val_acc: 0.6952\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.70050\n",
            "Epoch 46/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.0848 - acc: 0.9707 - val_loss: 1.7742 - val_acc: 0.6830\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.70050\n",
            "Epoch 47/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.0841 - acc: 0.9705 - val_loss: 1.8111 - val_acc: 0.6830\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.70050\n",
            "Epoch 48/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.0711 - acc: 0.9752 - val_loss: 1.7428 - val_acc: 0.6957\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.70050\n",
            "Epoch 49/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.0750 - acc: 0.9734 - val_loss: 1.8060 - val_acc: 0.6872\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.70050\n",
            "Epoch 50/50\n",
            "390/390 [==============================] - 9s 24ms/step - loss: 0.0702 - acc: 0.9748 - val_loss: 1.8170 - val_acc: 0.6860\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.70050\n",
            "Model took 477.22 seconds to train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxiFeiQhqRRU",
        "colab_type": "text"
      },
      "source": [
        "The best model is printed and accuracy is printed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf0zvcK3Qh7s",
        "colab_type": "code",
        "outputId": "4924e253-e0ba-4d3e-c298-4247943add92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('saved.hdf5')\n",
        "score = model.evaluate(xtest, ytest, verbose=1)\n",
        "print(score)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 2s 153us/step\n",
            "[1.5031730436325073, 0.7005]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}