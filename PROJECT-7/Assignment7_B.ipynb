{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment7-B.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhawalnegi1/EVA/blob/master/PROJECT-7/Assignment7_B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps4HLdNgmR0G",
        "colab_type": "text"
      },
      "source": [
        "Import all requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkwXnw9OfHZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "% matplotlib inline\n",
        "np.random.seed(2017)\n",
        "\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation,SeparableConv2D,GlobalAveragePooling2D,Lambda\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Reshape,Input,Lambda\n",
        "from keras.layers.merge import concatenate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-mZkGZPmnc4",
        "colab_type": "text"
      },
      "source": [
        "Import the cifar dataset and store in the variables for training and test purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHpnoCHZfO8g",
        "colab_type": "code",
        "outputId": "e7ed1c35-50ea-409b-e744-f8d3945d3a46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load CIFAR10 Data\n",
        "from keras.datasets import cifar10\n",
        "(xtrain, ytrain), (xtest, ytest) = cifar10.load_data()\n",
        "img_height, img_width, channel = xtrain.shape[1],xtrain.shape[2],xtrain.shape[3]\n",
        "num_classes = len(np.unique(ytest))\n",
        "print(num_classes)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK9qBBDDmyTn",
        "colab_type": "text"
      },
      "source": [
        "Function accuracy is defined to calculate the validation accuracy by finding total no of images classfied correctly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Adhd5AMrD3RP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(xtest, ytest, model):\n",
        "    result = model.predict(xtest)\n",
        "    predicted_class = np.argmax(result, axis=1)\n",
        "    true_class = np.argmax(ytest, axis=1)\n",
        "    num_correct = np.sum(predicted_class == true_class) \n",
        "    accuracy = float(num_correct)/result.shape[0]\n",
        "    return (accuracy * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxjI7Pl8m0a6",
        "colab_type": "text"
      },
      "source": [
        "Pixel Normalisation is done for training and testing data. And the class labels for training and testing data is converted into one-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5c5nDvxm6zR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xtrain = xtrain.astype('float32')/255\n",
        "xtest = xtest.astype('float32')/255\n",
        "# convert class labels to binary class labels\n",
        "ytrain = np_utils.to_categorical(ytrain, num_classes)\n",
        "ytest = np_utils.to_categorical(ytest, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeP0qOhfEkiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def space_to_depth_x2(x):\n",
        "    return tf.space_to_depth(x, block_size=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l2Ey2mAm3DS",
        "colab_type": "text"
      },
      "source": [
        "##Block1 - Normal Convolution\n",
        "\n",
        "2 layers of normal 3x3 convolution is add and after that bottleneck layer of 1x1 and maxpooling is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q5Rxgakv4SG",
        "colab_type": "code",
        "outputId": "5387bc83-2012-48da-8685-9df16e93d246",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "inp = Input(shape=(img_height, img_width, channel,))\n",
        "\n",
        "L1 = SeparableConv2D(32, (3,3), strides=(1,1), use_bias=False,border_mode='same')(inp)\n",
        "L1 = BatchNormalization()(L1)\n",
        "L1 = Activation('relu')(L1)\n",
        "L1 = SeparableConv2D(64, (3,3), strides=(1,1), use_bias=False,border_mode='same')(L1)\n",
        "L1 = BatchNormalization()(L1)\n",
        "L1 = Activation('relu')(L1)\n",
        "\n",
        "L2 = Conv2D(32, (3,3), strides=(1,1), use_bias=False,border_mode='same')(L1)\n",
        "L2 = BatchNormalization()(L2)\n",
        "L2 = Activation('relu')(L2)\n",
        "L2 = Conv2D(64, (3,3), strides=(1,1), use_bias=False,border_mode='same')(L2)\n",
        "L2 = BatchNormalization()(L2)\n",
        "L2 = Activation('relu')(L2)\n",
        "\n",
        "L3 = Conv2D(32, (3,3), strides=(1,1), use_bias=False,border_mode='same')(L2)\n",
        "L3 = BatchNormalization()(L3)\n",
        "L3 = Activation('relu')(L3)\n",
        "L3 = Conv2D(64, (3,3), strides=(1,1), use_bias=False,border_mode='same')(L3)\n",
        "L3 = BatchNormalization()(L3)\n",
        "L3 = Activation('relu')(L3)\n",
        "\n",
        "C1 = concatenate([L1,L3])\n",
        "\n",
        "L4 = SeparableConv2D(32, (3,3), strides=(1,1), use_bias=False,padding='same')(C1)\n",
        "L4 = BatchNormalization()(L4)\n",
        "L4 = Activation('relu')(L4)\n",
        "L4 = SeparableConv2D(64, (3,3), strides=(1,1), use_bias=False,padding='same')(L4)\n",
        "L4 = BatchNormalization()(L4)\n",
        "L4 = Activation('relu')(L4)\n",
        "\n",
        "C2 = concatenate([L1,L4])\n",
        "\n",
        "MAX1 = MaxPooling2D(pool_size=(2, 2))(C2)\n",
        "\n",
        "L5 = SeparableConv2D(32, (3,3), strides=(1,1), use_bias=False,padding='same')(MAX1)\n",
        "L5 = BatchNormalization()(L5)\n",
        "L5 = Activation('relu')(L5)\n",
        "\n",
        "l1= Lambda(space_to_depth_x2)(L1)\n",
        "l4= Lambda(space_to_depth_x2)(L4)\n",
        "C2 = concatenate([l1,l4,L5])\n",
        "\n",
        "L6 = Conv2D(32, (3,3), strides=(1,1), use_bias=False,padding='same')(C2)\n",
        "L6 = BatchNormalization()(L6)\n",
        "L6 = Activation('relu')(L6)\n",
        "L6 = Conv2D(64, (3,3), strides=(1,1), use_bias=False,padding='same')(L6)\n",
        "L6 = BatchNormalization()(L6)\n",
        "L6 = Activation('relu')(L6)\n",
        "\n",
        "l3= Lambda(space_to_depth_x2)(L3)\n",
        "l2= Lambda(space_to_depth_x2)(L2)\n",
        "C3 = concatenate([l3,l4,L5,L6])\n",
        "\n",
        "L7 = SeparableConv2D(32, (3,3), strides=(1,1), use_bias=False,padding='same')(C3)\n",
        "L7 = BatchNormalization()(L7)\n",
        "L7 = Activation('relu')(L7)\n",
        "\n",
        "C3 = concatenate([l1,l3,l4,L5,L6,L7])\n",
        "\n",
        "L8 = SeparableConv2D(32, (3,3), strides=(1,1), use_bias=False,padding='same')(C3)\n",
        "L8 = BatchNormalization()(L8)\n",
        "L8 = Activation('relu')(L8)\n",
        "L8 = SeparableConv2D(64, (3,3), strides=(1,1), use_bias=False,padding='same')(L8)\n",
        "L8 = BatchNormalization()(L8)\n",
        "L8 = Activation('relu')(L8)\n",
        "\n",
        "C4 = concatenate([l1,l4,L5,L7,L8])\n",
        "\n",
        "MAX2 = MaxPooling2D(pool_size=(2, 2))(C4)\n",
        "\n",
        "l6= Lambda(space_to_depth_x2)(L6)\n",
        "C5 = concatenate([l6,MAX2])\n",
        "\n",
        "L9 = Conv2D(32, (3,3), strides=(1,1), use_bias=False,padding='same')(C5)\n",
        "L9 = BatchNormalization()(L9)\n",
        "L9 = Activation('relu')(L9)\n",
        "L9 = Conv2D(64, (3,3), strides=(1,1), use_bias=False,padding='same')(L9)\n",
        "L9 = BatchNormalization()(L9)\n",
        "L9 = Activation('relu')(L9)\n",
        "\n",
        "l1= Lambda(space_to_depth_x2)(l1)\n",
        "l2= Lambda(space_to_depth_x2)(l2)\n",
        "l4= Lambda(space_to_depth_x2)(l4)\n",
        "l7= Lambda(space_to_depth_x2)(L7)\n",
        "C6 = concatenate([l1,l2,l4,l7,L9])\n",
        "\n",
        "L10 = SeparableConv2D(32, (3,3), strides=(1,1), use_bias=False,padding='same')(C6)\n",
        "L10 = BatchNormalization()(L10)\n",
        "L10 = Activation('relu')(L10)\n",
        "L10 = SeparableConv2D(64, (3,3), strides=(1,1), use_bias=False,padding='same')(L10)\n",
        "L10 = BatchNormalization()(L10)\n",
        "L10 = Activation('relu')(L10)\n",
        "\n",
        "l3= Lambda(space_to_depth_x2)(l3)\n",
        "l5= Lambda(space_to_depth_x2)(L5)\n",
        "C7 = concatenate([l2,l3,l5,L9,L10])\n",
        "\n",
        "L11 = Conv2D(32, (3,3), strides=(1,1), use_bias=False,padding='same')(C7)\n",
        "L11 = BatchNormalization()(L11)\n",
        "L11 = Activation('relu')(L11)\n",
        "L11 = Conv2D(64, (3,3), strides=(1,1), use_bias=False,padding='same')(L11)\n",
        "L11 = BatchNormalization()(L11)\n",
        "L11 = Activation('relu')(L11)\n",
        "\n",
        "C8 = concatenate([l3,l4,l5,l7,L10,L11])\n",
        "\n",
        "L12 = SeparableConv2D(32, (3,3), strides=(1,1), use_bias=False,padding='same')(C8)\n",
        "L12 = BatchNormalization()(L12)\n",
        "L12 = Activation('relu')(L12)\n",
        "L12 = SeparableConv2D(64, (3,3), strides=(1,1), use_bias=False,padding='same')(L12)\n",
        "L12 = BatchNormalization()(L12)\n",
        "L12 = Activation('relu')(L12)\n",
        "\n",
        "C9 = concatenate([l4,l7,L10,L12])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0618 17:12:50.094331 139656125454208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0618 17:12:50.130024 139656125454208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `SeparableConv2D` call to the Keras 2 API: `SeparableConv2D(32, (3, 3), strides=(1, 1), use_bias=False, padding=\"same\")`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "W0618 17:12:50.139378 139656125454208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0618 17:12:50.184921 139656125454208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0618 17:12:50.185687 139656125454208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0618 17:12:53.101676 139656125454208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `SeparableConv2D` call to the Keras 2 API: `SeparableConv2D(64, (3, 3), strides=(1, 1), use_bias=False, padding=\"same\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), strides=(1, 1), use_bias=False, padding=\"same\")`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), strides=(1, 1), use_bias=False, padding=\"same\")`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), strides=(1, 1), use_bias=False, padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), strides=(1, 1), use_bias=False, padding=\"same\")`\n",
            "W0618 17:12:53.855526 139656125454208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syzaQVIfpxZy",
        "colab_type": "text"
      },
      "source": [
        "Finally the no of channels are reduced to 10 and flatten and softmax is applied."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpp2vTc8EN_M",
        "colab_type": "code",
        "outputId": "f236456d-e794-47a2-d1d7-9e2de882cf2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "OUTPUT = Conv2D(10,8)(C9)\n",
        "print(OUTPUT)\n",
        "OUTPUT = GlobalAveragePooling2D()(OUTPUT)\n",
        "print(OUTPUT)\n",
        "Output = Activation('softmax')(OUTPUT)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"conv2d_11/BiasAdd:0\", shape=(?, 1, 1, 10), dtype=float32)\n",
            "Tensor(\"global_average_pooling2d_1/Mean:0\", shape=(?, 10), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSOb2lkJfhVq",
        "colab_type": "code",
        "outputId": "c0319ebb-bf3a-472d-82ca-714204079188",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3893
        }
      },
      "source": [
        "model= Model(inputs=[inp],outputs=[Output])\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_1 (SeparableCo (None, 32, 32, 32)   123         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 32)   128         separable_conv2d_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 32)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_2 (SeparableCo (None, 32, 32, 64)   2336        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 64)   256         separable_conv2d_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 64)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 32)   18432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 32)   128         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 32)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 64)   18432       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 64)   256         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 64)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 32)   18432       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 64)   18432       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 64)   256         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 128)  0           activation_2[0][0]               \n",
            "                                                                 activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_3 (SeparableCo (None, 32, 32, 32)   5248        concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 32)   128         separable_conv2d_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 32)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_4 (SeparableCo (None, 32, 32, 64)   2336        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 64)   256         separable_conv2d_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 128)  0           activation_2[0][0]               \n",
            "                                                                 activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 128)  0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_5 (SeparableCo (None, 16, 16, 32)   5248        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         separable_conv2d_5[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 16, 16, 256)  0           activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 32)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 16, 16, 256)  0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 16, 16, 544)  0           lambda_1[0][0]                   \n",
            "                                                                 lambda_2[0][0]                   \n",
            "                                                                 activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 16, 16, 32)   156672      concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 16, 16, 64)   18432       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 16, 16, 256)  0           activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 64)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 16, 16, 608)  0           lambda_3[0][0]                   \n",
            "                                                                 lambda_2[0][0]                   \n",
            "                                                                 activation_9[0][0]               \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_6 (SeparableCo (None, 16, 16, 32)   24928       concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         separable_conv2d_6[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 16, 16, 896)  0           lambda_1[0][0]                   \n",
            "                                                                 lambda_3[0][0]                   \n",
            "                                                                 lambda_2[0][0]                   \n",
            "                                                                 activation_9[0][0]               \n",
            "                                                                 activation_11[0][0]              \n",
            "                                                                 activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_7 (SeparableCo (None, 16, 16, 32)   36736       concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         separable_conv2d_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 32)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_8 (SeparableCo (None, 16, 16, 64)   2336        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 64)   256         separable_conv2d_8[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 64)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 16, 16, 640)  0           lambda_1[0][0]                   \n",
            "                                                                 lambda_2[0][0]                   \n",
            "                                                                 activation_9[0][0]               \n",
            "                                                                 activation_12[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 8, 8, 256)    0           activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 640)    0           concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 8, 8, 896)    0           lambda_5[0][0]                   \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 8, 8, 32)     258048      concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 8, 32)     128         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 8, 8, 32)     0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 8, 8, 64)     18432       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 16, 16, 256)  0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, 8, 8, 1024)   0           lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_9 (Lambda)               (None, 8, 8, 128)    0           activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, 8, 8, 1024)   0           lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, 8, 8, 1024)   0           lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 8, 8, 64)     0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 8, 8, 3264)   0           lambda_6[0][0]                   \n",
            "                                                                 lambda_7[0][0]                   \n",
            "                                                                 lambda_8[0][0]                   \n",
            "                                                                 lambda_9[0][0]                   \n",
            "                                                                 activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_9 (SeparableCo (None, 8, 8, 32)     133824      concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 8, 8, 32)     128         separable_conv2d_9[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 8, 8, 32)     0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_10 (SeparableC (None, 8, 8, 64)     2336        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         separable_conv2d_10[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 8, 8, 64)     0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_10 (Lambda)              (None, 8, 8, 1024)   0           lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_11 (Lambda)              (None, 8, 8, 128)    0           activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 8, 8, 2304)   0           lambda_7[0][0]                   \n",
            "                                                                 lambda_10[0][0]                  \n",
            "                                                                 lambda_11[0][0]                  \n",
            "                                                                 activation_16[0][0]              \n",
            "                                                                 activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 8, 8, 32)     663552      concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 8, 8, 32)     128         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 8, 8, 32)     0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 8, 8, 64)     18432       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 8, 8, 64)     256         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 8, 8, 64)     0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 8, 8, 2432)   0           lambda_10[0][0]                  \n",
            "                                                                 lambda_8[0][0]                   \n",
            "                                                                 lambda_11[0][0]                  \n",
            "                                                                 lambda_9[0][0]                   \n",
            "                                                                 activation_18[0][0]              \n",
            "                                                                 activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_11 (SeparableC (None, 8, 8, 32)     99712       concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 8, 8, 32)     128         separable_conv2d_11[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 8, 8, 32)     0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_12 (SeparableC (None, 8, 8, 64)     2336        activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 8, 8, 64)     256         separable_conv2d_12[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 8, 8, 64)     0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 8, 8, 1280)   0           lambda_8[0][0]                   \n",
            "                                                                 lambda_9[0][0]                   \n",
            "                                                                 activation_18[0][0]              \n",
            "                                                                 activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 1, 1, 10)     819210      concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 10)           0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 10)           0           global_average_pooling2d_1[0][0] \n",
            "==================================================================================================\n",
            "Total params: 2,348,101\n",
            "Trainable params: 2,346,053\n",
            "Non-trainable params: 2,048\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fh77A9fqBLC",
        "colab_type": "text"
      },
      "source": [
        "Model is compiled and summery is printed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvVs5tC8mR6Q",
        "colab_type": "code",
        "outputId": "ceebc2c1-a736-4c63-a2d3-664b98e5a25d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0618 17:12:55.402479 139656125454208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3gHMtq1p_5J",
        "colab_type": "text"
      },
      "source": [
        "Model is Trained for 50 epocs and checkpoint is used to saved the model with best validation accuracy and time taken to run is calculated. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oK_V7WyfsXX",
        "colab_type": "code",
        "outputId": "4910745d-a534-4827-bc23-389ed81c3f4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6939
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(zoom_range=0.0, \n",
        "                             horizontal_flip=False)\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
        "# train the model\n",
        "start = time.time()\n",
        "# Train the model\n",
        "filepath=\"saved.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "model_info = model.fit_generator(datagen.flow(xtrain, ytrain, batch_size = 128),\n",
        "                                 samples_per_epoch = xtrain.shape[0], nb_epoch = 100, \n",
        "                                 validation_data = (xtest,ytest),callbacks= callbacks_list, verbose=1)\n",
        "end = time.time()\n",
        "print (\"Model took %0.2f seconds to train\"%(end - start))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=(array([[[..., callbacks=[<keras.ca..., verbose=1, steps_per_epoch=390, epochs=100)`\n",
            "W0618 17:12:55.535671 139656125454208 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "390/390 [==============================] - 72s 184ms/step - loss: 7.8231 - acc: 0.2362 - val_loss: 10.4262 - val_acc: 0.1774\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.17740, saving model to saved.hdf5\n",
            "Epoch 2/100\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 1.7076 - acc: 0.5201 - val_loss: 11.4586 - val_acc: 0.1479\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.17740\n",
            "Epoch 3/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 1.5894 - acc: 0.6081 - val_loss: 2.6543 - val_acc: 0.5526\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.17740 to 0.55260, saving model to saved.hdf5\n",
            "Epoch 4/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 1.1634 - acc: 0.6737 - val_loss: 1.1710 - val_acc: 0.6638\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.55260 to 0.66380, saving model to saved.hdf5\n",
            "Epoch 5/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.9799 - acc: 0.7189 - val_loss: 1.1665 - val_acc: 0.6801\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.66380 to 0.68010, saving model to saved.hdf5\n",
            "Epoch 6/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.9372 - acc: 0.7436 - val_loss: 5.3800 - val_acc: 0.2934\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.68010\n",
            "Epoch 7/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.8982 - acc: 0.7572 - val_loss: 0.9380 - val_acc: 0.6894\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.68010 to 0.68940, saving model to saved.hdf5\n",
            "Epoch 8/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.7775 - acc: 0.7847 - val_loss: 0.8528 - val_acc: 0.7239\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.68940 to 0.72390, saving model to saved.hdf5\n",
            "Epoch 9/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.6274 - acc: 0.8226 - val_loss: 0.8854 - val_acc: 0.7246\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.72390 to 0.72460, saving model to saved.hdf5\n",
            "Epoch 10/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 1.0805 - acc: 0.7506 - val_loss: 0.8961 - val_acc: 0.7215\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.72460\n",
            "Epoch 11/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.6731 - acc: 0.8218 - val_loss: 1.0803 - val_acc: 0.7131\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.72460\n",
            "Epoch 12/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.5502 - acc: 0.8554 - val_loss: 0.9488 - val_acc: 0.7326\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.72460 to 0.73260, saving model to saved.hdf5\n",
            "Epoch 13/100\n",
            "390/390 [==============================] - 63s 160ms/step - loss: 0.4651 - acc: 0.8804 - val_loss: 1.0864 - val_acc: 0.7202\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.73260\n",
            "Epoch 14/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.3846 - acc: 0.9019 - val_loss: 1.0869 - val_acc: 0.7215\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.73260\n",
            "Epoch 15/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.3454 - acc: 0.9143 - val_loss: 1.0944 - val_acc: 0.7317\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.73260\n",
            "Epoch 16/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.2905 - acc: 0.9339 - val_loss: 1.4420 - val_acc: 0.6888\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.73260\n",
            "Epoch 17/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.2354 - acc: 0.9486 - val_loss: 1.4626 - val_acc: 0.7141\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.73260\n",
            "Epoch 18/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.2386 - acc: 0.9472 - val_loss: 1.3049 - val_acc: 0.7333\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.73260 to 0.73330, saving model to saved.hdf5\n",
            "Epoch 19/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.2309 - acc: 0.9493 - val_loss: 5.1504 - val_acc: 0.4676\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.73330\n",
            "Epoch 20/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.1431 - acc: 0.9541 - val_loss: 1.4712 - val_acc: 0.7245\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.73330\n",
            "Epoch 21/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0553 - acc: 0.9828 - val_loss: 1.3922 - val_acc: 0.7388\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.73330 to 0.73880, saving model to saved.hdf5\n",
            "Epoch 22/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.0322 - acc: 0.9910 - val_loss: 1.4796 - val_acc: 0.7295\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.73880\n",
            "Epoch 23/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.0280 - acc: 0.9927 - val_loss: 1.6962 - val_acc: 0.7173\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.73880\n",
            "Epoch 24/100\n",
            "390/390 [==============================] - 63s 163ms/step - loss: 0.0582 - acc: 0.9799 - val_loss: 1.7104 - val_acc: 0.7119\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.73880\n",
            "Epoch 25/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.0784 - acc: 0.9719 - val_loss: 1.6205 - val_acc: 0.7241\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.73880\n",
            "Epoch 26/100\n",
            "390/390 [==============================] - 63s 163ms/step - loss: 0.0554 - acc: 0.9803 - val_loss: 1.7411 - val_acc: 0.7252\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.73880\n",
            "Epoch 27/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.0331 - acc: 0.9898 - val_loss: 1.5978 - val_acc: 0.7389\n",
            "\n",
            "Epoch 00027: val_acc improved from 0.73880 to 0.73890, saving model to saved.hdf5\n",
            "Epoch 28/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0407 - acc: 0.9864 - val_loss: 1.7435 - val_acc: 0.7135\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.73890\n",
            "Epoch 29/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0536 - acc: 0.9817 - val_loss: 1.9144 - val_acc: 0.7069\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.73890\n",
            "Epoch 30/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0621 - acc: 0.9785 - val_loss: 1.7648 - val_acc: 0.7178\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.73890\n",
            "Epoch 31/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0341 - acc: 0.9887 - val_loss: 1.7900 - val_acc: 0.7218\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.73890\n",
            "Epoch 32/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0242 - acc: 0.9923 - val_loss: 1.7034 - val_acc: 0.7305\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.73890\n",
            "Epoch 33/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0256 - acc: 0.9918 - val_loss: 1.7737 - val_acc: 0.7253\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.73890\n",
            "Epoch 34/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.0432 - acc: 0.9847 - val_loss: 1.9751 - val_acc: 0.7089\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.73890\n",
            "Epoch 35/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.0569 - acc: 0.9795 - val_loss: 1.8374 - val_acc: 0.7253\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.73890\n",
            "Epoch 36/100\n",
            "390/390 [==============================] - 63s 163ms/step - loss: 0.0396 - acc: 0.9861 - val_loss: 1.7997 - val_acc: 0.7286\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.73890\n",
            "Epoch 37/100\n",
            "390/390 [==============================] - 63s 163ms/step - loss: 0.0234 - acc: 0.9923 - val_loss: 1.7606 - val_acc: 0.7319\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.73890\n",
            "Epoch 38/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.0188 - acc: 0.9939 - val_loss: 1.8583 - val_acc: 0.7285\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.73890\n",
            "Epoch 39/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0331 - acc: 0.9881 - val_loss: 1.8735 - val_acc: 0.7245\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.73890\n",
            "Epoch 40/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0384 - acc: 0.9872 - val_loss: 1.8794 - val_acc: 0.7282\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.73890\n",
            "Epoch 41/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0360 - acc: 0.9875 - val_loss: 1.8967 - val_acc: 0.7315\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.73890\n",
            "Epoch 42/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0227 - acc: 0.9927 - val_loss: 1.8623 - val_acc: 0.7307\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.73890\n",
            "Epoch 43/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.0162 - acc: 0.9949 - val_loss: 2.1995 - val_acc: 0.7153\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.73890\n",
            "Epoch 44/100\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 0.0377 - acc: 0.9870 - val_loss: 2.1112 - val_acc: 0.7075\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.73890\n",
            "Epoch 45/100\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 0.0367 - acc: 0.9868 - val_loss: 1.8861 - val_acc: 0.7264\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.73890\n",
            "Epoch 46/100\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 0.0186 - acc: 0.9939 - val_loss: 1.8449 - val_acc: 0.7318\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.73890\n",
            "Epoch 47/100\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 0.0136 - acc: 0.9956 - val_loss: 1.9593 - val_acc: 0.7277\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.73890\n",
            "Epoch 48/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0271 - acc: 0.9903 - val_loss: 2.2466 - val_acc: 0.7137\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.73890\n",
            "Epoch 49/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0446 - acc: 0.9843 - val_loss: 1.9524 - val_acc: 0.7251\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.73890\n",
            "Epoch 50/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0288 - acc: 0.9899 - val_loss: 2.1335 - val_acc: 0.7118\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.73890\n",
            "Epoch 51/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0162 - acc: 0.9946 - val_loss: 1.9066 - val_acc: 0.7386\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.73890\n",
            "Epoch 52/100\n",
            "390/390 [==============================] - 63s 160ms/step - loss: 0.0136 - acc: 0.9958 - val_loss: 2.2792 - val_acc: 0.6987\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.73890\n",
            "Epoch 53/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0220 - acc: 0.9925 - val_loss: 1.9668 - val_acc: 0.7262\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.73890\n",
            "Epoch 54/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0312 - acc: 0.9892 - val_loss: 2.1450 - val_acc: 0.7150\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.73890\n",
            "Epoch 55/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.0280 - acc: 0.9904 - val_loss: 2.1481 - val_acc: 0.7249\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.73890\n",
            "Epoch 56/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0142 - acc: 0.9949 - val_loss: 1.9938 - val_acc: 0.7343\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.73890\n",
            "Epoch 57/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0096 - acc: 0.9968 - val_loss: 1.9977 - val_acc: 0.7277\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.73890\n",
            "Epoch 58/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0210 - acc: 0.9930 - val_loss: 2.2737 - val_acc: 0.7056\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.73890\n",
            "Epoch 59/100\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 0.0296 - acc: 0.9902 - val_loss: 2.1493 - val_acc: 0.7203\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.73890\n",
            "Epoch 60/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0331 - acc: 0.9887 - val_loss: 2.0756 - val_acc: 0.7227\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.73890\n",
            "Epoch 61/100\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 0.0158 - acc: 0.9947 - val_loss: 1.9372 - val_acc: 0.7394\n",
            "\n",
            "Epoch 00061: val_acc improved from 0.73890 to 0.73940, saving model to saved.hdf5\n",
            "Epoch 62/100\n",
            "390/390 [==============================] - 63s 163ms/step - loss: 0.0103 - acc: 0.9963 - val_loss: 1.9767 - val_acc: 0.7394\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.73940\n",
            "Epoch 63/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.0100 - acc: 0.9970 - val_loss: 2.0819 - val_acc: 0.7315\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.73940\n",
            "Epoch 64/100\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 0.0202 - acc: 0.9935 - val_loss: 2.2162 - val_acc: 0.7213\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.73940\n",
            "Epoch 65/100\n",
            "390/390 [==============================] - 64s 164ms/step - loss: 0.0287 - acc: 0.9906 - val_loss: 2.1674 - val_acc: 0.7274\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.73940\n",
            "Epoch 66/100\n",
            "390/390 [==============================] - 64s 163ms/step - loss: 0.0183 - acc: 0.9940 - val_loss: 2.0445 - val_acc: 0.7372\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.73940\n",
            "Epoch 67/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.0182 - acc: 0.9934 - val_loss: 2.1384 - val_acc: 0.7292\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.73940\n",
            "Epoch 68/100\n",
            "390/390 [==============================] - 63s 163ms/step - loss: 0.0142 - acc: 0.9948 - val_loss: 2.1003 - val_acc: 0.7317\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.73940\n",
            "Epoch 69/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.0105 - acc: 0.9965 - val_loss: 2.0382 - val_acc: 0.7421\n",
            "\n",
            "Epoch 00069: val_acc improved from 0.73940 to 0.74210, saving model to saved.hdf5\n",
            "Epoch 70/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0187 - acc: 0.9936 - val_loss: 2.3538 - val_acc: 0.7150\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.74210\n",
            "Epoch 71/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.0180 - acc: 0.9940 - val_loss: 2.1623 - val_acc: 0.7245\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.74210\n",
            "Epoch 72/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.0154 - acc: 0.9949 - val_loss: 2.0585 - val_acc: 0.7388\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.74210\n",
            "Epoch 73/100\n",
            "390/390 [==============================] - 63s 162ms/step - loss: 0.0166 - acc: 0.9941 - val_loss: 2.2965 - val_acc: 0.7175\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.74210\n",
            "Epoch 74/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0179 - acc: 0.9940 - val_loss: 2.3674 - val_acc: 0.7165\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.74210\n",
            "Epoch 75/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0161 - acc: 0.9945 - val_loss: 2.1401 - val_acc: 0.7325\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.74210\n",
            "Epoch 76/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0197 - acc: 0.9931 - val_loss: 2.1908 - val_acc: 0.7278\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.74210\n",
            "Epoch 77/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0121 - acc: 0.9955 - val_loss: 2.1547 - val_acc: 0.7324\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.74210\n",
            "Epoch 78/100\n",
            "390/390 [==============================] - 63s 160ms/step - loss: 0.0105 - acc: 0.9964 - val_loss: 2.3473 - val_acc: 0.7127\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.74210\n",
            "Epoch 79/100\n",
            "390/390 [==============================] - 63s 160ms/step - loss: 0.0117 - acc: 0.9960 - val_loss: 2.2168 - val_acc: 0.7358\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.74210\n",
            "Epoch 80/100\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 0.0195 - acc: 0.9928 - val_loss: 2.2918 - val_acc: 0.7231\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.74210\n",
            "Epoch 81/100\n",
            "390/390 [==============================] - 63s 161ms/step - loss: 0.0210 - acc: 0.9927 - val_loss: 2.0914 - val_acc: 0.7421\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.74210\n",
            "Epoch 82/100\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 0.0136 - acc: 0.9955 - val_loss: 2.1660 - val_acc: 0.7279\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.74210\n",
            "Epoch 83/100\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 0.0111 - acc: 0.9961 - val_loss: 2.1400 - val_acc: 0.7328\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.74210\n",
            "Epoch 84/100\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 0.0114 - acc: 0.9960 - val_loss: 2.2152 - val_acc: 0.7290\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.74210\n",
            "Epoch 85/100\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 0.0155 - acc: 0.9949 - val_loss: 2.2662 - val_acc: 0.7286\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.74210\n",
            "Epoch 86/100\n",
            "390/390 [==============================] - 63s 160ms/step - loss: 0.0171 - acc: 0.9943 - val_loss: 2.1693 - val_acc: 0.7289\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.74210\n",
            "Epoch 87/100\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 0.0162 - acc: 0.9946 - val_loss: 2.3278 - val_acc: 0.7251\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.74210\n",
            "Epoch 88/100\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 0.0133 - acc: 0.9959 - val_loss: 2.2527 - val_acc: 0.7316\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.74210\n",
            "Epoch 89/100\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 0.0084 - acc: 0.9972 - val_loss: 2.1309 - val_acc: 0.7347\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.74210\n",
            "Epoch 90/100\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 0.0109 - acc: 0.9968 - val_loss: 2.3949 - val_acc: 0.7212\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.74210\n",
            "Epoch 91/100\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 0.0180 - acc: 0.9940 - val_loss: 2.5852 - val_acc: 0.7025\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.74210\n",
            "Epoch 92/100\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 0.0159 - acc: 0.9946 - val_loss: 2.2956 - val_acc: 0.7227\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.74210\n",
            "Epoch 93/100\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 0.0113 - acc: 0.9962 - val_loss: 2.2034 - val_acc: 0.7355\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.74210\n",
            "Epoch 94/100\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 0.0086 - acc: 0.9969 - val_loss: 2.2333 - val_acc: 0.7307\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.74210\n",
            "Epoch 95/100\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 0.0071 - acc: 0.9975 - val_loss: 2.2925 - val_acc: 0.7379\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.74210\n",
            "Epoch 96/100\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 0.0145 - acc: 0.9952 - val_loss: 2.2839 - val_acc: 0.7279\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.74210\n",
            "Epoch 97/100\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 0.0156 - acc: 0.9945 - val_loss: 2.2798 - val_acc: 0.7258\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.74210\n",
            "Epoch 98/100\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 0.0086 - acc: 0.9971 - val_loss: 2.2871 - val_acc: 0.7321\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.74210\n",
            "Epoch 99/100\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 0.0073 - acc: 0.9972 - val_loss: 2.1933 - val_acc: 0.7420\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.74210\n",
            "Epoch 100/100\n",
            "390/390 [==============================] - 62s 160ms/step - loss: 0.0112 - acc: 0.9962 - val_loss: 2.3429 - val_acc: 0.7285\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.74210\n",
            "Model took 6306.12 seconds to train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf0zvcK3Qh7s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9e79f955-e3b2-4c7c-fd97-ccb005b5634e"
      },
      "source": [
        "score = model.evaluate(xtest, ytest, verbose=1)\n",
        "print(score)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 7s 662us/step\n",
            "[2.3429378987312317, 0.7285]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}